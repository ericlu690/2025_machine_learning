import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# 函數與導數
def runge(x):
    return 1 / (1 + 25 * x**2)

def runge_derivative(x):
    return -50 * x / (1 + 25 * x**2)**2

# 訓練資料
x_train = torch.linspace(-1, 1, 101).unsqueeze(1)
y_train = runge(x_train)
y_prime_train = runge_derivative(x_train)
x_train.requires_grad_(True)

# 神經網路
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(1, 20)
        self.output = nn.Linear(20,1)
        self.act = torch.tanh
    def forward(self, x):
        return self.output(self.act(self.hidden(x)))

net = Net()
criterion = nn.MSELoss()
alpha = 0.5

# 訓練函數
def train_net(net, lr, epochs, step_plot=120):
    optimizer = optim.SGD(net.parameters(), lr=lr)
    for epoch in range(1, epochs+1):
        optimizer.zero_grad()
        y_pred = net(x_train)
        
        y_pred_prime = torch.autograd.grad(
            outputs=y_pred,
            inputs=x_train,
            grad_outputs=torch.ones_like(y_pred),
            create_graph=True
        )[0]
        
        loss_f = criterion(y_pred, y_train)
        loss_df = criterion(y_pred_prime, y_prime_train)
        loss_total = alpha*loss_f + (1-alpha)*loss_df
        
        loss_total.backward()
        optimizer.step()
        
        if epoch % step_plot == 0:
            print(f"Epoch {epoch}: Loss_f={loss_f.item():.4f}, Loss_df={loss_df.item():.4f}, Loss_total={loss_total.item():.4f}")
            
            # 圖 1: 原函數
            plt.figure(figsize=(5,4))
            plt.scatter(x_train.detach().numpy(), y_train.detach().numpy(), label="True f(x)", s=15)
            plt.plot(x_train.detach().numpy(), y_pred.detach().numpy(), 'r-', label="NN f(x)")
            plt.title(f"Epoch {epoch}: Function")
            plt.legend()
            plt.show()
            
            # 圖 2: 導數
            plt.figure(figsize=(5,4))
            plt.scatter(x_train.detach().numpy(), y_prime_train.detach().numpy(), label="True f'(x)", s=15)
            plt.plot(x_train.detach().numpy(), y_pred_prime.detach().numpy(), 'g-', label="NN f'(x)")
            plt.title(f"Epoch {epoch}: Derivative")
            plt.legend()
            plt.show()

# 階段 1: lr=0.2, 240 次
train_net(net, lr=0.2, epochs=240, step_plot=120)

# 階段 2: lr=0.02, 120 次
train_net(net, lr=0.02, epochs=120, step_plot=120)

# 階段 3: lr=0.005, 240 次
train_net(net, lr=0.005, epochs=240, step_plot=120)
