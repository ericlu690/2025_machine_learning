(provided by discussing with ChatGPT and run by colab)
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import matplotlib.pyplot as plt

# =============================
# 1. 讀取分類資料
# =============================
data = pd.read_csv("classification_dataset.csv")

X = torch.tensor(data.iloc[:, :2].values, dtype=torch.float32)
y = torch.tensor(data.iloc[:, -1].values, dtype=torch.float32).view(-1,1)

# =============================
# 2. 手動拆分 train/test (8:2)
# =============================
num_samples = X.shape[0]
indices = torch.randperm(num_samples)
train_size = int(0.8 * num_samples)

train_idx = indices[:train_size]
test_idx = indices[train_size:]

X_train, y_train = X[train_idx], y[train_idx]
X_test, y_test = X[test_idx], y[test_idx]

# =============================
# 3. 定義 MLP 模型
# =============================
class MLPBinaryClassifier(nn.Module):
    def __init__(self, input_dim=2, hidden_dim=16):
        super(MLPBinaryClassifier, self).__init__()
        self.hidden = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.output = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.hidden(x)
        x = self.relu(x)
        logits = self.output(x)
        return logits

model = MLPBinaryClassifier(input_dim=2, hidden_dim=16)

# =============================
# 4. 損失函數
# =============================
criterion = nn.BCEWithLogitsLoss()

# =============================
# 5. 訓練函數（每5次輸出 Loss 並紀錄 Accuracy）
# =============================
def train_phase(model, X_train, y_train, X_test, y_test, lr, epochs, start_epoch=0):
    optimizer = optim.SGD(model.parameters(), lr=lr)
    epoch_list = []
    loss_list = []
    train_acc_list = []
    test_acc_list = []

    for epoch in range(1, epochs+1):
        logits = model(X_train)
        loss = criterion(logits, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if epoch % 5 == 0:
            train_pred = torch.sigmoid(model(X_train)) >= 0.5
            train_acc = (train_pred.float() == y_train).float().mean().item()

            test_pred = torch.sigmoid(model(X_test)) >= 0.5
            test_acc = (test_pred.float() == y_test).float().mean().item()

            print(f"Epoch {start_epoch+epoch:3d} | LR={lr} | Loss: {loss.item():.6f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}")

            epoch_list.append(start_epoch + epoch)
            loss_list.append(loss.item())
            train_acc_list.append(train_acc)
            test_acc_list.append(test_acc)

    return epoch_list, loss_list, train_acc_list, test_acc_list

# =============================
# 6. 執行兩段訓練
# =============================
# Phase 1: lr=0.01, 100 epochs
epochs1, loss1, train_acc1, test_acc1 = train_phase(model, X_train, y_train, X_test, y_test, lr=0.01, epochs=100, start_epoch=0)

# Phase 2: lr=0.001, 100 epochs
epochs2, loss2, train_acc2, test_acc2 = train_phase(model, X_train, y_train, X_test, y_test, lr=0.001, epochs=100, start_epoch=100)

# 合併兩段結果
epochs_total = epochs1 + epochs2
loss_total = loss1 + loss2
train_acc_total = train_acc1 + train_acc2
test_acc_total = test_acc1 + test_acc2

# =============================
# 7. 繪製 Loss 與 Accuracy 曲線
# =============================
plt.figure(figsize=(12,5))

# Loss 曲線
plt.subplot(1,2,1)
plt.plot(epochs_total, loss_total, 'b-o')
plt.xlabel("Epoch")
plt.ylabel("BCE Loss")
plt.title("Training Loss Curve")
plt.grid(True)

# Accuracy 曲線
plt.subplot(1,2,2)
plt.plot(epochs_total, train_acc_total, 'b-o', label="Train Acc")
plt.plot(epochs_total, test_acc_total, 'r-o', label="Test Acc")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy Curve")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
